{
    "BERT": {
        "checkpoints": ["bert-base-cased", "roberta-base"],
        "learning_rates": [1e-5, 2e-5, 3e-5],
        "batch_sizes": [16,  24, 32],
        "num_epochs": [7, 10],
        "weight_decay": [0.01],
        "output_dir": "results/bert"
    },
    "LLAMA": {
        "quantization": [true, false],
        "temperature": [0.3, 0.5, 0.7, 0.9],
        "top_p": [0.6, 0.7, 0.8],
        "output_dir": "results/LLAMA"
    },
    "LSTM": {
        "dropout_rate": [0.3, 0.4],
        "batch_sizes": [32, 128],
        "num_epochs": [20],
        "learning_rates": [1e-3, 1e-2],
        "output_dir": "results/lstm"
    },
    "GPT-4": {
        "p_value": [0.05, 0.1, 0.15],
        "temperature": [0.3, 0.5, 0.7, 0.9],
        "output_dir": "results/gpt-4"
    }

}

